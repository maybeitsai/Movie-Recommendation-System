# -*- coding: utf-8 -*-
"""Movie-Recommendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u31Ez7Oq1pV_x-xnrvSG3XIY0_2yP4ul

# Data Understanding

## Import Library
"""

# Commented out IPython magic to ensure Python compatibility.
# Import library yang dibutuhkan
import pandas as pd
import numpy as np
import seaborn as sns
import random
import matplotlib.pyplot as plt
# %matplotlib inline
import warnings
warnings.filterwarnings("ignore")

"""## Data Loading"""

# Load datasets
movies = pd.read_csv('assets\movie\movies.csv')
ratings = pd.read_csv('assets\movie\\ratings.csv')

"""## Exploratory Data Analysis (EDA)

### movies.csv

#### Menampilkan dataset movies
"""

# Tampilkan dataset movies
movies

"""#### Menghitung jumlah film"""

print("Jumlah film:",len(movies['movieId'].unique()))

"""#### Melakukan visualisasi jumlah film per genre"""

# Visualisasi Jumlah Film per Genre
genre_count = movies['genres'].str.split('|', expand=True).stack().value_counts()
plt.figure(figsize=(10,6))
sns.barplot(x=genre_count.values, y=genre_count.index, palette='muted')
plt.title('Jumlah Film per Genre')
plt.xlabel('Jumlah Film')
plt.ylabel('Genre')
plt.show()

"""### ratings.csv

#### Menampilkan dataset ratings
"""

# Tampilkan dataset ratings
ratings

"""#### Menghitung Jumlah user dan film"""

# Menghitung Jumlah user dan film
print("Jumlah user:",len(ratings['userId'].unique()),"\nJumlah film:",len(ratings['movieId'].unique()))

"""#### Menampilkan statistik deskriptif ratings"""

# Statistik Deskriptif ratings
ratings.describe()

"""#### Melakukan visualisasi rating film"""

# Visualisasi Distribusi Rating
plt.figure(figsize=(10,6))
sns.countplot(x='rating', data=ratings, palette='viridis')
plt.title('Distribusi Rating Film')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.show()

"""#### Menganalisis jumlah rating per tahun"""

# Analisis Waktu
ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')
ratings['year'] = ratings['timestamp'].dt.year

plt.figure(figsize=(10,6))
sns.countplot(x='year', data=ratings, palette='Set3')
plt.title('Jumlah Rating per Tahun')
plt.xlabel('Tahun')
plt.ylabel('Jumlah Rating')
plt.show()

"""# Content-Based Filtering

## Data Preparation

### Menyalin dataset movies
"""

# Mengcopy dataset movie
movies_df = movies.copy()

"""### Membuat kolom baru berdasarkan genre"""

# Membuat kolom baru untuk setiap genre
genres = set()
for genre_string in movies_df['genres']:
    genres.update(genre_string.split('|'))

# Inisialisasi kolom dengan nilai 0
for genre in genres:
    movies_df[genre] = 0

# Mengisi nilai kolom baru
for index, row in movies_df.iterrows():
    for genre in row['genres'].split('|'):
        movies_df.at[index, genre] = 1

"""### Menghapus baris yang tidak memiliki genre"""

# Menghapus baris yang tidak memiliki genre
movies_df = movies_df[movies_df['genres'] != '(no genres listed)']

"""### Menampilkan data yang sudah diolah"""

# Menampilkan dataset
movies_df

"""### Menghapus kolom yang tidak diperlukan"""

movies_df.drop('(no genres listed)', axis=1, inplace=True)

"""### Menampilkan kolom pada data"""

movies_df.columns

"""## Model Development

### Menghitung cosine similarity
"""

from sklearn.metrics.pairwise import cosine_similarity
# Hitung similarity score
similarity_matrix = cosine_similarity(movies_df.iloc[:, 3:])  # Menghitung similarity score dari kolom genre

"""### Membuat fungsi untuk rekomendasi"""

# Buat fungsi untuk mendapatkan rekomendasi
def get_recommendations(movie_title, similarity_matrix=similarity_matrix, movies_df=movies_df, top_n=10):
    idx = movies_df.index[movies_df['title'] == movie_title].tolist()[0] # Dapatkan indeks film berdasarkan judul
    similarity_scores = list(enumerate(similarity_matrix[idx])) # Dapatkan similarity scores dari film dengan film lainnya
    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True) # Urutkan similarity scores dari yang tertinggi
    top_similar_movies = similarity_scores[1:top_n+1] # Ambil top N film yang paling mirip
    recommended_movies = [(movies_df.iloc[movie[0]]['title'], movie[1]) for movie in top_similar_movies] # Dapatkan judul dari film-film yang direkomendasikan

    return recommended_movies

"""### Simulasi penggunaan sistem rekomendasi"""

# Memilih judul film secara acak dari dataset
random_movie_title = random.choice(movies_df['title'])
random_movie_genre = movies_df.loc[movies_df['title'] == random_movie_title, 'genres'].values[0]

# Mendapatkan rekomendasi untuk film tersebut
recommendations = get_recommendations(random_movie_title, similarity_matrix, movies_df)

# Menampilkan judul dan genre dari film yang direkomendasikan
print(f"Judul: {random_movie_title}")
print(f"Genre: {random_movie_genre}")
print("======"*7)
print("TOP 10 REKOMENDASI FILM")
print("======"*7)
print
number = 1
for movie, score in recommendations:
    genres = movies_df[movies_df['title'] == movie]['genres'].values[0]
    print(f"{number}.) {movie}, Genre: {genres})")
    number+=1

"""# Collaborative Filtering

## Cluster Based Algorithm

### Data Preparation

#### Melakukan import library yang dibutuhkan
"""

from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error

"""#### Menggabungkan dataset ratings dan movies"""

# Menggabungkan dataset ratings dan movies
data = pd.merge(ratings, movies, on='movieId')

"""#### Membuat pivot table untuk user-item"""

# Membuat pivot table untuk user-item
user_item_matrix = data.pivot_table(index='userId', columns='movieId', values='rating')

"""#### Mengisi nilai null dengan 0"""

# Mengisi nilai null dengan 0
user_item_matrix = user_item_matrix.fillna(0)

"""#### Menampilkan matrix rating"""

# Menampilkan user-item matrix
user_item_matrix

"""### Model development

#### Membuat pipeline
"""

# Menggunakan Pipeline untuk menangani proses standarisasi dan clustering
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('kmeans', KMeans())
])

"""#### Membuat parameter untuk dilakukan hyperparameter tuning"""

# Mengatur parameter untuk grid search
param_grid = {
    'kmeans__n_clusters': list(range(5, 21)),
    'kmeans__init': ['random'],
    'kmeans__n_init': [5, 10, 15],
    'kmeans__max_iter': [100, 150, 200],
    'kmeans__tol': [0.00001, 0.0001, 0.001],
    'kmeans__random_state': [42]
}

"""#### Melakukan hyperparameter tuning dengan metode GridSearch"""

# Melakukan grid search untuk menemukan parameter terbaik
grid_search = GridSearchCV(pipeline, param_grid, cv=5, verbose=2)
grid_search.fit(user_item_matrix)

"""#### Mendapatkan parameter terbaik"""

# Menampilkan parameter terbaik
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best Parameters:", best_params)
print("Best Score:", best_score)

"""#### Melakukan clustering dengan parameter terbaik"""

# Melakukan clustering dengan parameter terbaik
best_kmeans = KMeans(n_clusters=best_params['kmeans__n_clusters'], init=best_params['kmeans__init'])
clusters = best_kmeans.fit_predict(user_item_matrix)

"""#### Menambahkan kolom 'cluster' pada matrix"""

# Menambahkan kolom 'cluster' pada user_item_matrix
user_item_matrix['cluster'] = clusters

"""#### Membuat fungsi rekomendasi film"""

# Membuat fungsi untuk mendapatkan rekomendasi
def recommend_movies_kmeans(user_id, num_recommendations=10):
    user_cluster = user_item_matrix.loc[user_id]['cluster']
    cluster_movies = user_item_matrix[user_item_matrix['cluster'] == user_cluster].drop(columns='cluster')
    user_ratings = user_item_matrix.loc[user_id].drop('cluster')
    recommended_movies = cluster_movies.mean().sort_values(ascending=False)
    already_rated = user_ratings[user_ratings > 0].index
    recommended_movies = recommended_movies.drop(already_rated)
    recommended_movies = recommended_movies.reset_index()
    recommended_movies = pd.merge(recommended_movies, movies, left_on='movieId', right_on='movieId')

    # Menghitung MSE dan RMSE
    actual_ratings = user_item_matrix.loc[user_id].drop('cluster')
    predicted_ratings = cluster_movies.mean()
    predicted_ratings = predicted_ratings[actual_ratings.index]
    mse = mean_squared_error(actual_ratings, predicted_ratings)
    rmse = np.sqrt(mse)
    print("MSE:", mse)
    print("RMSE:", rmse)

    return recommended_movies[['movieId', 'title', 'genres']].head(num_recommendations)

"""#### Simulasi penggunaan sistem rekomendasi"""

# Contoh penggunaan sistem rekomendasi film
user_id = np.random.randint(1, user_item_matrix.shape[0] + 1)
print("TOP 10 REKOMENDASI FILM UNTUK USER", user_id, ":")
recommend_movies_kmeans(user_id)

"""## Deep Learning

### Data Preparation

#### Melakukan import library yang dibutuhkan
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.metrics import RootMeanSquaredError
from sklearn.model_selection import train_test_split
from kerastuner import HyperModel
from kerastuner.tuners import RandomSearch

"""#### Menggabungkan dataset ratings dan movies"""

# Menyiapkan dataset
data_dl = pd.merge(ratings, movies, on='movieId')

"""#### Menampilkan dataset"""

# Menampilkan dataset
data_dl

"""#### Melakukan proses encoding pada userId dan movieId"""

# Mengubah userId menjadi list tanpa nilai yang sama
user_ids = data_dl['userId'].unique().tolist()
print('list userID: ', user_ids)

# Melakukan encoding userId
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke userId
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

# Mengubah movieId menjadi list tanpa nilai yang sama
movie_ids = data_dl['movieId'].unique().tolist()

# Melakukan proses encoding movieId
movie_to_movie_encoded = {x: i for i, x in enumerate(movie_ids)}

# Melakukan proses encoding angka ke movieId
movie_encoded_to_movie = {i: x for i, x in enumerate(movie_ids)}

"""#### Melakukan mapping pada data"""

# Mapping userID ke dataframe user
data_dl['user'] = data_dl['userId'].map(user_to_user_encoded)

# Mapping movieId ke dataframe movie
data_dl['movie'] = data_dl['movieId'].map(movie_to_movie_encoded)

"""#### Mencari jumlah user dan movie serta nilai maksimum dan minimum rating"""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)

# Mendapatkan jumlah movie
num_movie = len(movie_encoded_to_movie)

# Mengubah rating menjadi nilai float
data_dl['rating'] = data_dl['rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(data['rating'])

# Nilai maksimal rating
max_rating = max(data['rating'])

print('Number of User: {}, Number of Movie: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_movie, min_rating, max_rating
))

"""#### Melakukan teknik one-hot encoding"""

# Melakukan one-hot encoding user dan movie
user_encoded = pd.get_dummies(data_dl['user'], prefix='user')
movie_encoded = pd.get_dummies(data_dl['movie'], prefix='movie')

# Menggabungkan user dan movie
encoded_data = pd.concat([user_encoded, movie_encoded], axis=1)

"""#### Menentukan variabel x dan y"""

# Membuat variabel input X
X = encoded_data.values
# Membuat variabel y untuk membuat rating dari hasil
y = data['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

"""#### Membagi dataset untuk training dan testing"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

"""### Model development

#### Membangun model
"""

# Definisikan model
class RecommenderHyperModel(HyperModel):
    def __init__(self, num_users, num_movies):
        self.num_users = num_users
        self.num_movies = num_movies

    def build(self, hp):
        model = keras.Sequential()
        model.add(layers.Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu', input_dim=self.num_users + self.num_movies))
        model.add(layers.Dropout(rate=hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))
        model.add(layers.Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'))
        model.add(layers.Dropout(rate=hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))
        model.add(layers.Dense(1, activation='sigmoid'))

        model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-3, 1e-4])),
                      loss=MeanSquaredError(),
                      metrics=[RootMeanSquaredError()])

        return model

"""#### Membuat fungsi callback"""

# Membuat ThresholdLossCallback
class ThresholdLossCallback(Callback):
    def __init__(self, threshold):
        super(ThresholdLossCallback, self).__init__()
        self.threshold = threshold

    def on_epoch_end(self, epoch, logs=None):
        if logs.get('root_mean_squared_error') is not None and logs.get('root_mean_squared_error') < self.threshold:
            print(f"\nTraining stopped as root_mean_squared_error ({logs.get('root_mean_squared_error'):.4f}) has reached the threshold ({self.threshold})")
            self.model.stop_training = True

rmse_threshold_callback = ThresholdLossCallback(threshold=0.01)

# Fungsi callback yang digunakan
callbacks = [
    EarlyStopping(monitor='root_mean_squared_error', patience=10, verbose=1, restore_best_weights=True),
    ModelCheckpoint(filepath='best_model.h5', monitor='root_mean_squared_error', save_best_only=True, verbose=1),
    ReduceLROnPlateau(factor=0.5, patience=5, min_lr=0.000001, verbose=1), #
    rmse_threshold_callback
]

"""#### Melakukan hyperparameter tuning"""

# Melakukan hyperparameter tuning
hypermodel = RecommenderHyperModel(num_users, num_movie)

tuner = RandomSearch(
    hypermodel,
    objective='val_loss',
    max_trials=50,
    executions_per_trial=1,
    directory='my_dir',
    project_name='recommender_system'
)

tuner.search_space_summary()

tuner.search(X_train, y_train,
             epochs=5,
             batch_size=128,
             validation_data=(X_test, y_test))

"""#### Membuat model dari hyperparameter terbaik"""

# Mengambil model terbaik dari hyperparameter tuning
best_model = tuner.get_best_models(num_models=1)[0]
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
best_model.summary()

"""#### Mengcompile dan training model"""

# Compile dan latih model terbaik
best_model.compile(optimizer=Adam(learning_rate=best_hps.get('learning_rate')),
                   loss=MeanSquaredError(),
                   metrics=[RootMeanSquaredError()])

history = best_model.fit(X_train, y_train,
                         epochs=100,
                         validation_data=(X_test, y_test),
                         callbacks=callbacks,
                         batch_size=128)

"""#### Visualisasi Root Mean Squared Error (RMSE)"""

# Membuat plot grafik metrics
plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""#### Mempersiapkan data movies dengan id yang baru"""

# Membuat kolom ID baru dengan hasil encoding movieId
movies['new_movieId'] = movies['movieId'].map(movie_to_movie_encoded)

# Menangani nilai non-finite dengan menggantinya dengan nilai -1
movies['new_movieId'] = movies['new_movieId'].fillna(-1)

# Mengonversi kolom ke tipe data integer
movies['new_movieId'] = movies['new_movieId'].astype(int)

# Tampilkan data movies setelah penambahan kolom ID baru dan penghapusan nilai kosong
movies[['movieId', 'title', 'new_movieId']]

"""#### Membuat fungsi untuk mendapatkan rekomendasi"""

# Membuat fungsi untuk rekomendasi
def recommend_movies_dl(user_id,n):
    unwatched_movies = data_dl[data_dl['user'] != user_id]['movie'].unique() # Filter film-film yang belum ditonton oleh pengguna
    user_input = np.array([user_to_user_encoded[user_id]] * len(unwatched_movies)) # Buat input untuk model berdasarkan pengguna yang akan direkomendasikan dan film-film yang belum ditonton
    movie_input = np.array(unwatched_movies)
    user_encoded_input = tf.keras.utils.to_categorical(user_input, num_classes=num_users) # Buat one-hot encoded input untuk user
    movie_encoded_input = tf.keras.utils.to_categorical(movie_input, num_classes=num_movie) # Buat one-hot encoded input untuk movie
    X_recommend = np.concatenate((user_encoded_input, movie_encoded_input), axis=1) # Gabungkan kedua input
    predicted_ratings = best_model.predict(X_recommend).flatten() # Prediksi rating untuk film-film yang belum ditonton
    unwatched_movie_ids = [movie_encoded_to_movie[movie_id] for movie_id in movie_input] # Filter film-film yang sudah ditonton oleh pengguna
    unwatched_movies_df = movies[movies['new_movieId'].isin(unwatched_movie_ids)] # Buat DataFrame untuk film-film yang belum ditonton

    # Gabungkan data rating yang diprediksi dengan data film-film yang belum ditonton
    recommendations = pd.DataFrame({'movieId': unwatched_movie_ids, 'predicted_rating': predicted_ratings})
    recommendations = pd.merge(recommendations, unwatched_movies_df, on='movieId')
    # Urutkan film-film berdasarkan prediksi rating tertinggi
    top_n_recommendations = recommendations.sort_values(by='predicted_rating', ascending=False).head(n)

    return top_n_recommendations.reset_index(drop=True)

"""#### Penggunaan sistem rekomendasi"""

# Menentukan pengguna yang akan direkomendasikan
user_id = random.choice(user_ids)
# Melakukan rekomendasi
recommendations = recommend_movies_dl(user_id,10)
print(f"Top 10 Film Rekomendasi Untuk User {user_id}:")
recommendations[['movieId','title', 'genres']]